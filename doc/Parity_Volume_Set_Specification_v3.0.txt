Parity Volume Set Specificiation 3.0

Michael Nahas

Started January 16th, 2020


Based on Parity Volume Set Specification 2.0 [2003-05-11] by Michael Nahas with ideas from Peter Clements, Paul Nettle, and Ryan Gallagher
Based on Parity Volume Set Specification 1.0 [2001-10-14] by Stefan Wehlus and others.


Introduction:

This document describes a file format for storing redundant data for a set of files.  If any of the original files is damaged in storage or transmission, the redundant data can be used to regenerate the original input.  Of course, not all damages can be repaired, but many can. 

In operation, a user will select a set of files from which the redundant data is to be made.  These are known as "input files" and together they are known as the "input set". The user will provide these to a program which generates files that match the specification in this document.  The program is known as a "PAR 3.0 client" or "client" for short, and the generated files are known as "PAR 3.0 files" or "PAR files". If the files in the input set ever get damaged (e.g. when they are transmitted unreliably or stored on a faulty disk) the client can read the damaged input files, read the (possibly damaged) PAR files, and regenerate the original input files.  Again, not all damages can be repaired, but many can. 

In addition to being a file format, the Par 3.0 standard can be used as a network protocol for forward error correction.  Instead of files, data objects can be packaged in the Par 3.0 format and send over an unreliable channel, like UDP.  Redundant data can be generated and also sent over the channel.  The receiver can use the redundant data to recover data objects that experienced data loss.


Design Goals:

Par 3.0's goal is to provide a complete solution for the bottom two layers of archiving - redundant data and splitting.  The other layers are best supported by other programs (tar, zip/gzip/7zip, pgp/gpg, etc.).  Par 3.0 does provide minimal support for other layers, for ease and integration.  

Major differences from Par 2.0 are:
* support any systematic linear code (Reed-Solomon with Vandermonde matrix or Cauchy, LDPC, random sparse martix).
* support streaming / single-pass recovery.
* support files that work both as a PAR 3.0 file and another type.  For example, putting recovery data inside a ZIP, ISO 9600, or other file.

Part of "support any systematic linear code" is to fix the major bug in Par 2.0: it did not do Reed-Solomon encoding as it promised.  There was a major mistake in the paper that Par 2.0 relied on.  The problem manifested as a bug in Par 1.0 and, while Par 2.0 reduced its occurance, it did not fix the problem.  Par 2.0 did not use an always invertible matrix; it essentially used a random matrix, which (luckily) is invertible with high probability.  Par 3.0 fixes that bug.

The other part of "support any systematic linear code" is that non-Reed-Solomon codes can be much faster.  LDPC and sparse random matrices will speed things up dramatically, with a slight increase in errors that cannot be recovered from.

Some minor differences from Par 2.0 are:
* UTF-8 filenames (which were supported as a never-published Par 2.1 standard)
* support for recovering multiple PAR files with overlapping input file sets
* the ability to change file names without regenerating every packet
* support for empty directories
* support for more than 2^16 files

(NOTE: The choice to focus on a protocol that could be used for streaming was a big one.  An alternate design would be to create a filesystem that supported redundancy.  E.g., a derivative of BTRFS with an additional tree of redundancy blocks.  That approach is an interesting direction, for any open source designers out there.)


The "Big Picture":

Par uses Linear Algebra to generate the redundant data and, after damage, to recover the original input data.  To understand how it works, you need to be familiar with vectors, matrices, etc.

The calculation of redundant data starts with the input data being packaged into a set of vectors.  Those vectors are multiplied by the "code matrix" to generate vectors of redundant data.  Thus, an input vector "i" times the code matrix "C" generates a redundant vector "r".

r = iC

Recovery is accomplished by first identifying the good input data and good redundant data.  Good data is data that arrived intact; bad data was lost or damaged.  We can then permute and partition of the elements of each vector into "good" ones and "bad" ones.  We can do the same for the elements of the code matrix.

r = | r_good |   i = | i_good |    C = | C_good,good C_good,bad | 
    | r_bad  |       | i_bad  |        | C_bad,good  C_bad,bad  |

| r_good | = r = iC = | i_good || C_good,good C_good,bad |
| r_bad  |            | i_bad  || C_bad,good  C_bad,bad  |            


Our goal, of course, is to recover the bad input data.  To do that, we pull out the equation for the good redundant data...

r_good = i_good*C_good,good + i_bad*C_good,bad

... and solve for the bad input data.

i_bad = (r_good - i_good*C_good,good) C_good,bad^-1

Here, "^-1" indicates the right inverse of the matrix.  Since the redundant data is made by multiplying the input data by the code matrix, the inverse of the code matrix allows us to recreate the missing input data from the redundant data.  Not every matrix has a right inverse.  When the right inverse does not exist, we cannot recover the input data.  A right inverse never exists when the matrix has more columns than rows, which means we cannot recover if there are more bad input blocks than good redundant blocks.  

Unlike your linear algebra class, the elements of the vectors and matrices are not real numbers or complex numbers, but elements of a "Galois Field".  Like the computer's integers (a.k.a., the integers mod 2^N), they come in various sizes like 8-bits, 16-bits, etc. and support operations called addition, subtraction, multiplication, and division.  Unlike the computer's integers, "division" exactly inverts "multiplication" for every value.  (Computer integers can overflow during multiplication, preventing division from inverting the multiplication.)  That perfect inversion allows the linear algebra to work.

Par 3.0 improves on Par 2.0 by supporting multiple Galois Fields and by supporting any code matrix.  This means Par 3.0 supports a large set of error correcting codes known as "linear systematic codes".  These include Reed-Solomon and many Low Density Parity Check (LDPC) codes.  This flexibility allows Par 3.0 clients to choose between speed and the number of errors that can be recovered.  It also allows Par 3.0 to support any codes whose patents expire or new codes that are developed.


The "Detailed Picture":

Generating the recovery data starts with choosing a Galois Field and block size.  The Galois Field is usually chosen based on whatever is fastest for the computer's hardware.  For this example, I'll assume it fits in 2-bytes (16-bits).  The block size is the smallest unit for recovering data.  It is usually chosen to match the transmission/storage technology or to limit overhead.  The block size must be a multiple of the size of the Galois Field.  For this example, I'll assume it is 2048 bytes, but in practice it can be much larger, often megabytes.

Next, the input files are mapped onto a "single virtual file".  The single virtual file contains all the protected data and the mappings tell where each part of each input file is located in the single virtual file.  Any part of the single virtual file that doesn't correspond to part of an input file is assumed to be filled with zero bytes.  

The single virtual input file is then broken into equal-sized blocks.  For this example, the blocks are each 2048 bytes long.  If the single virtual file is N blocks long, each of those N blocks can be seen as holding 1024 Galois Field values (each 2-byte in size).

The next step reorganizes the blocks into vectors.  The N blocks containing 1024 Galois Field values becomes 1024 vectors containing N Galois Field values.  This is done the obvious way: swapping rows for columns and columns for rows.  The values in the i-th block become the i-th element of each vector; the j-th values in each block are used to make the j-th vector.   

Next, the user chooses the numbers of recovery blocks that they want. The number of recovery blocks determines the maximum number of damaged/missing input blocks that we can recover.  Often the number of recovery blocks is 5% or 10% of the number of input blocks. 

Next, is the choice of the code matrix.  The code matrix has a column for each input block and a row for each recovery block.  The elements of the matrix can be anything --- PAR 3.0 supports any systematic linear code.  Codes vary in speed and the probability of recoverying from an error.

Next, for each input vector, we make a recovery vector by multiplying the input vector with the code matrix.  There is only one code matrix; the code matrix is the same for every pair of vectors.  Since there is one recovery vector for each input vector, there are 1024 recovery vectors.  The length of the recovery vectors is equal to how many recovery blocks we want.  

The next step reorganizes the recovery vectors into recovery blocks.  It is basically the inverse of the step that reorganized the input blocks into input vectors.  If we want R recovery blocks, the 1024 recovery vectors of length R become R recovery blocks with 1024 Galois Field values.  The i-th element in each vector goes into the i-th recovery block; the j-th vector is used to make the j-th value in each recovery block.

The recovery blocks are stored in a Par 3.0 file or multiple PAR 3.0 files.  The input blocks can be stored in their original files or put in Par 3.0 file(s).  The PAR 3.0 files contain a checksum for every input and recovery block.  After the files are stored or transmitted, the checksums are used to determine which blocks are damaged.

After storage or transmission, if there are any input blocks missing or damaged, we need to do recovery:  First, we identify elements of the input vectors associated with the missing/damaged input blocks.  Next, we identify elements of the recovery vectors associated with the missing/damaged recovery block.  We then perform the math (described above) which uses the good elements of each recovery vector to recover the bad elements of its associated input vector.  The math requires inverting a submatrix of the code matrix and, if the right inverse does not exist, we fail.  

After recovering the missing elements of each input vector, the data is reorganized to regenerate the missing input blocks.  Those blocks are written into their associated files, which should complete the repair.  Par 3.0 has a checksum for each file to make sure that the repair process worked correctly.


File Format Basics:

The PAR 3.0 file itself is made of packets - self-contained parts with their own checksum. This design prevents damage to one part of the file from making the whole file unusable. 

Packets have a type and each type of packet serves a different purpose.  One type describes the code matrix.  Another contains the checksums of input blocks.  Yet another contains a recovery block.  There are many other types.

A PAR 3.0 file is only required to contain 1 specific packet - the packet that identifies the client that created the file. This way, if clients are creating files that don't match the specification in some way, they can be tracked down.

The packets can be packaged into multiple files. Files can contain duplicate packets - in fact, this is recommended for vital packets, such as the ones that describe the input files. Packets can appear in any order in a file, but there is a recommended order that allows some clients to recover the file(s) in a single pass.  



Conventions:

There are a number of conventions used in the design of this specification.

The data is 8-byte aligned. That is, every field starts on an index in the file which is congruent to zero, modulus 8.  (That is, address % 8 == 0)  This is because some memory systems function faster if 64-bit quantities are 8-byte aligned.  Note that a file could be corrupted (bytes inserted or deleted) and thrown off the alignment. 

All integers in this version of the spec are integers of 4, 8, or 16 bytes in length.

All integers are little endian. (This is the default on x86 and x86-64 CPUs.)  Signed integers are in 2's complement notation.  (This is the default on every major architecture.)  

Strings are not null-terminated.  This is to prevent hackers from using stack-overflow attacks.  In order to make a string 8-byte aligned, 1 to 7 zero bytes may be appended.  If an N-byte field contains an array, a null-terminated string can be created by copying the N-byte field into a character array of length N+1 and then the setting the N+1 character to '\0'.

The lengths of arrays and strings are often implicit.  For example, if a region is known to be 32 bytes and that region contains an 8-byte integer and a string, then the string is known to take up 24 bytes.  The string is then at least 17 bytes in length, since the 24 bytes contains 0 to 7 bytes of '\0' padding at the end.

All strings are UTF-8.  WARNING: Writers of OSX/MacOS clients must take special care with UTF-8 filenames!  Unicode has multiple ways to encode the same string.  An e with an accent mark can be encoded as a single character (U+00e9) or two characters, one for the e (U+0065) and one for the accent mark (U+0301).  Par 3.0 does not require a particular encoding.  Forcing a particular encoding is called "normalization" in the Unicode vocabulary.  Most file systems do not normalize filenames and just treat the UTF-8 as a sequence of bytes.  Par 3.0 follows their practice.  However, HFS+ was Apple's default filesystem from 1998 to 2017 and it normalizes every filename.  Thus, if a Par client writes a file with a UTF-8 filename, the HFS+ file system may change the filename.  Clients for OSX/MacOS should be aware of this possibility.  Apple's current default filesystem, APFS, does not do normalization.  

The lengths of files and locations in files are determined by 16-byte integers. 
(Hard drive size doubles every 1.5 years and is expected to exceed 8-byte integers before 2040.)  It is acceptable for clients to not support files (including the single virtual file) that are larger than 2^64.  Clients that do not support large files must inform the user if they encounter a PAR file containing too large a file.

The block size and matrix indices are 8-bytes integers.  In order to protect files with more than 2^64 bytes, users must choose larger block sizes.  

Par 3.0 uses multiple hash functions.  The "rolling hash" is CRC32C.  It is used to identify input blocks that are not in their expected location.  This hash is only 32 bits long and may not uniquely identify a block.  

For uniqueness, Par 3.0 uses the KangarooTwelve hash, a.k.a. K12 hash.  This is a longer hash with cryptographic properties.  The K12 hash of a block or file is assumed to be unique.

A collection of PAR files which should be processed together are identified by 16-byte value known as the Stream ID.  

Input files have a "file index", which is an 8-byte unsigned integer.  For most uses, the file indices will be 0, 1, 2, ... N-1.  The file index is used to differentiate between two files that have the identical contents, but are different files.  Files that do not contain data (directories, hardlinks, softlinks) do not have a file index.

Every byte of a PAR file is specified. There are no places to throw junk bytes that can be any value.  Padding, where needed, is specified to be zero bytes.  The order of items in all arrays is specified.

When discussing vectors and matrices, this document uses zero-indexing.  That is, the elements in a vector are at locations 0 through N-1.  (One-indexing, the usual convention, has them at locations 1 through N.)


Description:

A PAR 3.0 file consists of a sequence of "packets". A packet has a fixed-sized header and a variable length body. The packet header contains a checksum for the packet - if the packet is damaged, the packet is ignored. The packet header also contains a packet-type. If the client does not understand the packet type, the packet is ignored. To be compliant with this specification, a client must understand the "core" set of packets. Client may process the optional packets or create their own application-specific packets.


Packet Header

Table: Packet Header
Length (bytes)	Type	Description
8	byte[8]	Magic sequence. Used to quickly identify location of packets. Value = {'P', 'A', 'R', '3', '\0', 'P', 'K', 'T'} (ASCII)
8	8-byte uint	Length of the entire packet. Must be multiple of 8. (NB: Includes length of header.)
16	K12 hash	K12 Hash of packet. Used as a checksum for the packet. Calculation starts at first byte of Stream ID and ends at last byte of body. Does not include the magic sequence, length field or this field.  NB: The K12 Hash, by its definition, includes the length as if it were appended to the packet.  !!!! IS THIS TRUE?
16	K12 hash	StreamID.  All packets that belong together have the same Stream ID. (See below for how it is calculated.)
16	byte[16]	Type. Can be anything. All beginning "PAR " (ASCII) are reserved for specification-defined packets. Application-specific packets are recommended to begin with the ASCII name of the client.
?*8	?	Body of Packet. Must be a multiple of 8 bytes.

The StreamID is used to identify packets that should be processed together, even if those packets were written to separate PAR files or arrived via different transmission methods.  A StreamID is associated with a unique single virtual file together with the block size and Galois Field.  If the contents of the single virtual file are known, it could be a K12 hash of the single virtual file along with the block size and Galois Field parameters of the SingleVirtualFilePacket.  This is impossible in streaming or single-pass situations.  In those cases, it should be a globally unique random number.  One way to generate the random number is the K12 hash of the triple consisting of: a machine identifier, a process identifier, and a high-resolution timestamp.

There are various types of packets. The "core" set of packets - the set of packets that all clients must recognize and process - are listed next. For each, the value for the "type" field will be listed along with the contents of the body of the packet. 


Creator packet

This packet is used to identify the client that created the file.  It is required to be in every PAR file.  If a client is unable to recover the input set due to a badly created file, the contents of the creator packet must be shown to the user.  The goal of this is that any client incompatibilities can be found and resolved quickly.

The creator packet has a type value of "PAR 2.0\0Creator\0" (ASCII). The packet's body contains the following:

Table: Creator Packet Body Contents
Length (bytes)	Type	Description
?*8	UTF-8 char array	UTF-8 text identifying the client.  This should also include a way to contact the client's creator - either through a URL or an email address.  Reminder: This is not a null terminated string.

It is recommended that the text in the creator packet include any parameters used to generate the file.  For example, the command line arguments.  This will aid in debugging problems.


Single Virtual File packet

This packet sets the block size and Galois Field.  It also identifies the parent stream, if there is one.

The creator packet has a type value of "PAR 3.0\0Matrix\0\0" (ASCII). The packet's body contains the following:

Table: Single Virtual File Packet Body Contents
Length (bytes)	Type	Description
8	8-byte uint	The size of the Galois field in bytes.
?*8	?-byte GF	The generator of the Galois field without its leading 1.
8	8-byte uint	block size in bytes
16      K12 hash        Stream ID of parent stream, or all zeros.
16	16-byte uint	Length of single virtual file in parent stream, or all zeros.

The generator is an element of the Galois field, written in little-endian format and padded with 0 to 7 zero bytes afterwards.  Thus, if the Galois field had a size of 2-bytes and a generator of 0x1100B, the first two bytes would hold the value 0x100B in little-endian format and the next 6 bytes would be zeroes.

Clients must support every possible Galois field. 

Clients are expected to optimize performance for specific Galois fields.  Some likely targets for optimization are:

Table: Common Galois Fields
Size (bits)    Generator (hexidecimal)
8              0x11B 
16             0x1100B
128            (1 << 128) + 0x43 

Note: The 8-bit Galois field is supported by the x86 instruction GF2P8MULB.

Note: The 16-bit Galois field is the same as in Par 2.0.

Note: 64-bit Galois fields are supported by the x86 instruction CLMUL.

Note: The 128-bit Galois field is implemented by Intel in this white paper:
https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/carry-less-multiplication-instruction-in-gcm-mode-paper.pdf

Note: The ARM processor has an instruction extension called "NEON" with a VMULL instruction.  VMULL.P8 will do eight 8-bit Galois field multiplications at once.

The block size must be a multiple of 8.  It must also be a multiple of the Galois Field size. 

The "parent stream" concept will mostly be used for incremental backups.  In this use case, there is an original backup of all files and then a incremental backup containing changes since the original.  The incremental backup is a different stream, but might want to reuse data in the single virtual file of the original backup.  By setting the final two fields, the incremental backup indicates that the first N bytes of its single virtual files are the same as the parent.  If the parent fields are set, the blocksize and Galois field must be the same as the parent's.  

NOTE: The "parent stream" concept includes NOT only copies the data in the single virtual file, but also the file indices, input file mapping, and input file meta data.  All packets from the parent stream should be processed as if part of the "child stream", with the exception of the Root packet.

For any Stream ID, there should be only 1 Single Virtual File Packet.  (The same packet may be repeated multiple times.)


Matrix Packets

There are currently 3 different types of matrix packets.  More than one packet can be used at the same time.  For example, a sparse matrix packet could be used to generate most recovery data and a Cauchy matrix packet to generate a few pieces of recovery data.  (This dual approach balances speed and recovery of errors.)

Cauchy Matrix Packet

This packet describes a Cauchy matrix.  It is used for all or part of the code matrix.  

The Cauchy matrix packet has a type value of "PAR 3.0\0Cauchy\0\0" (ASCII). The packet's body contains the following:

Table: Cauchy Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the packet header of the Single Virtual File Packet.
8	unsigned int	Number of zero columns
8	unsigned int	hint for number of rows 

The hint to the number of rows is used in single-pass situations to allocate buffers.  If the number of rows is unknown, the hint is set to zero.

To generate the element at zero-indexed row R and zero-indexed column C, the first check is if C is less than the number of zero columns.  If it is, then the element is zero.

Otherwise, the element at zero-indexed row R and zero-indexed column C depends on R and C.  Specifically, it is the multiplicative inverse of x_R-y_C, where x_R is the Galois field element with the same bit pattern as binary integer R+1 and y_C is the Galois field element associated with the binary integer MAX-C, where MAX is the maximum binary integer value with the same size as the Galois field.  (NOTE: In binary, MAX contains all ones.)  To be clear, the multiplicative inverse and subtraction x_R-y_C are done using Galois field arithmetic.  The R+1 addition and MAX-C subtraction is done using native binary integer arithmetic.


Sparse Random Matrix Packet

This packet describes a sparse random matrix.  It is faster to calculate recovery blocks with a sparse matrix, if the user is willing to accept a slightly higher chance of not recoverying from extreme cases.

To be specific, consider a sparse random matrix with N recovery blocks such that the matrix has at least N*ln(N) non-zero elements.  This matrix is able to recover from almost all cases where N - C input blocks are lost.  The probability of failing for a given C is propotional to 1/(g^C) where g is the number of unique the Galois field values.  Thus, for a 1-byte Galois field with 256 values, the probably that recovery fails with N-3 lost input blocks, is 1/(256^3) or less than one-in-a-million.  See "The Rank of Sparse Random Matrices over Finite Fields" by Blomer, Karp, and Welzl.

Recall that for any matrix, N+1 lost input blocks can never be recovered.  Using a sparse matrix, rather than a Cauchy, can be upto N/ln(N) times faster.  Users that can store/send 6 additional recovery blocks can get much faster performance with less than a 1-in-a-trillion increase in failure to recover.

The speed up and math work better for large numbers of recovery blocks.  When there are few recovery blocks, the Cauchy matrix is strongly recommended.

The matrix rows packet has a type value of "PAR 3.0\0Sparse\0\0" (ASCII). The packet's body contains the following:

Table: Sparse Random Matrix Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the packet header of the Single Virtual File Packet.
8	unsigned int	number of zero columns
8	unsigned int	number of rows 
8	unsigned int    lower bits of random number generator seed
8       unsigned int	number of non-zero elements per column

If the zero-indexed column number is less than the number of zero columns, then the column only contains zero Galois field values.

Otherwise, each column's non-zero elements are generated and then shuffled into position.  The following paragraphs describe the details.

First, the start of the column is filled with zeroes.  If the column has N elements (that is, the number of rows is N) and will have K non-zero elements, then the first N-K elements are set to zero.

Second, the pseudorandom number generator (PNRG) is initialized.  The generator is the PCG-XSL-RR, which has 128-bits of state and generates 64-bit pseudorandom numbers.  For each column of the matrix, the 128-bit seed is initialized with the higher 64-bits equal to the zero-indexed column of the matrix and the lower 64-bits equal to the seed value from the packet.

Third, non-zero elements are placed at the end of the column.  If there are K non-zero elements in a column, random non-zero Galois field values are placed in zero-indexed locations N-K-1 through N-1 of the column.  Random Galois field values are generated by filling them from the least-significant bytes to most significant using 64-bit values generated from the PRNG.  If the random value does not use all the bits of a value from the PRNG, the unused bits are discarded.  Since we want non-zero random values, if a zero is randomly generated, it is immediately discarded and whole new random values are generated.  Only when a non-zero value is generated do we move to the next location in the column.  This continues until the final K locations in the column have non-zero values.  

The last step in generating a random column is shuffling the non-zero values into their final location.  This is the usual O(N) shuffling algorithm, which swaps values into random locations.  (Wikipedia calls it "Fisher-Yates shuffle", others "Knuth shuffle".)  The first random location is in the range 0 to N-K-1 and is generated by taking a 64-bit unsigned value from the PRNG and applying modulus N-K.  The first non-random value, which is at location N-K-1, is then swapped with the value in the first random location.  (If the random location is N-K-1, then it "swaps with itself" and the column is not changed.)   The second random location is in the range 0 to N-K.  The second random non-zero value, at location N-K, is then swapped.  The third random location is in the range 0 to N-K+1.  Etc.  The final random non-zero value will be swapped to any location in the column.

The final result is a column with K random non-zero values in random locations.  This will mean data from from each input block will be incorporated into K different recovery blocks.  


Explicit Matrix Packet

This packet describes any single-row matrix.  The values of the elements are explicitly contained in the packet, and not computed like the other matrix packets.  It is intended to be used for LDPC, such as Tornado Codes.

Notice that this packet is row-based, while the sparse random matrix is column based.  LDPC repair is done on rows, rather than columns.  

The matrix rows packet has a type value of "PAR 3.0\0Explicit" (ASCII). The packet's body contains the following:

Table: Explicit Matrix Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the packet header of the Single Virtual File Packet.
?*?	{8-byte unsigned int, GF value, padding}	zero-indexed column location, Galois field value, 0 to 7 bytes of padding

The matrix contains just one row.  For each non-zero element of the row, there is a triple in the packet with its location, value, and 0 to 7 bytes of zero padding to make it 8-byte aligned.  The triples are in sorted order, with column locations increasing from lowest to highest.


Data Packet

This packet contains the data contents of the single virtual file.  This is how data from input files is included in the PAR file.

The data packet has a type value of "PAR 3.0\0Data\0\0\0\0" (ASCII). The packet's body contains the following:

Table: Data Packet Body Contents
Length (bytes)	Type	Description
16	unsigned int	The byte offset in the single virtual file.
4	unsigned int	The compression format.  (See below.)
12	K12 hash 	The hash of uncompressed data
?*8	unsigned int	The data itself.  

If the compression format code is 0, the data is uncompressed.  

If the compression format code is 1, the data is compressed with ZLIB.  

The uncompressed data's size must be a multiple of the blocksize.


!!!!! Should we allow the "hash of uncompressed data" to be zeroes if the data is uncompressed, since the packet already has a (longer) K12 hash checksum?



Data Checksum Packet

This packet contains checksums for blocks of the single virtual file.  It is used when the input files are sent outside the PAR files.

The data packet has a type value of "PAR 3.0\0BlkChkSm" (ASCII). The packet's body contains the following:

Table: Data Checksum Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the packet header of the Single Virtual File Packet.
16	unsigned int	The location of the first byte of the first block in the single virtual file.
?*16	{CRC32C, K12 hash} array	CRC32C and 12-byte K12 hash for a sequence of blocks in the single virtual file.  The CRC32C comes before the 12-byte K12 hash.

The CRC32C checksum is a rolling checksum.  It can be used to find block-sized pieces that have moved.

Notice that the K12 hash is only 12-bytes long here.  That is to keep the 8-byte alignment of data.  The K12 hash is used to uniquely identify the block, since CRC32C might not be unique.


Input File Data Integrity Packet

This packet holds the mapping of the input file to the single virtual file and a checksum for the protected data in the file.

The input file data integrity packet has a type value of "PAR 3.0\0IFDI\0\0\0\0" (ASCII). The packet's body contains the following:

Table: Input File Data Integrity Packet Body Contents
Length (bytes)	Type	Description
8	unsigned int	File index
16	unsigned int	File size
32	K12 hash	K12 hash of the protected section of the file
?*48	{index in input file, index in single virtual file, length} array	mapping of input file to single virtual file.  (See below.)

The file index are usually assigned sequentially 0, 1, 2, etc..  It uniquely identifies a "normal file".  ("Normal files" are not directories, symbolic links, etc.)  The file index also serves to make the checksum of this packet unique for files that have the same contents.  

The file checksum is only for the protected sections of the file, that is, those that are mapped to the single virtual file.  The checksum is for only those bytes in sequence, while completely skipping over all other parts of the file.  If only 10 bytes of 10 megabyte input file are mapped to the single virtual file, only 10 bytes are hashed.  If the file has no sections that are mapped, it is the hash of an empty file.

The mapping consists of triples, each representing a region mapped from the input file to the single virtual file.  The triples are: the location of the first byte in the input file, the location of the first byte in the single virtual file, and the length.  The triples are sorted by the first element: the location in the input file.  The regions specified by triples cannot overlap.  That is, each byte of the input file can only appear in a single region.  

The locations in the single virtual file do not have to be in order and they can overlap.  This allows deduplication between files and even inside the same file.  For example, if an input file was all the same value, it could be divided into many regions that all mapped to the same place on the single virtual file.


Input Subset Data Integrity Packet

This packet type is used to group input files that contain protected data.  In the old PAR 2.0 design, a checksum for every input file was put into a single packet.  Since PAR 3.0 supports 2^64 files, a single packet containing a checksums for every input file would be huge.  This packet type allows that long list to be broken up.

This packet type works by using a checksum-of-checksums.  Each file has a Input File Data Integrity packet.  The checksum of those packets are put into an Input Subset Data Integrity packet and the checksum of this packet is used to identify the group of files.  A large group can be referenced by putting the checksums of other Input Subset Data Integrity Packets into a packet.  The packets form a tree.  The checksum for the packet at the root of the tree --- which represents all the input files --- is placed in the Root Packet.

The input subset data integrity packet has a type value of "PAR 3.0\0ISDI\0\0\0\0" (ASCII). The packet's body contains the following:

Table: Input Subset Data Integrity Packet Body Contents
Length (bytes)	Type	Description
?*16	K12 hash	The checksums of the Input File Data Integrity Packets OR Input Subset Data Integrity Packets.

NOTE: The file index plays a role here.  If two files are duplicates of each other, the data contents will be exactly the same.  The file index will be the only thing different and it causes the duplicate files' Input File Data Integrity packets to have different checksums.

NOTE: The tree of these packets does NOT have to match the directory hierarchy.  It can, but checksums could also be packed in equal number into each packet.

If there is no parent stream, it is recommended that the files appear in file index order.  It is also recommended that the tree be "broad" and not "deep".  That is, the tree should not resemble a linked list.  This is so that streams that use it as a parent stream will be smaller.  (The child stream can reuse Input Subset Data Integrity Packets that do not change.)


Input File Metadata Packet

This packet contains metadata for an input file, including its filename. 

The recovery data packet has a type value of "PAR 3.0\0IFMeta\0\0" (ASCII). The packet's body contains the following:

Table: Input File Metadata Packet Body Contents
Length (bytes)	Type	Description
8      unsigned int 	Filename length
?*8    string		Filename, plus 0 to 7 zero bytes of padding
8      unsigned int 	Permission bits
8      signed int	Create time in nanos since Epoch
8      signed int	Last modification time in nanos since Epoch
?*8    ?		additional data

PAR 3.0 has rudimentary support for metadata.  The goal was to support most directory layouts for package distribution.  PAR 3.0 intentionally avoids requiring support for file ownership, to avoid any security issues.  If a user wants to store all the file permission that their file system supports, they should use a program specific to their file-system, like "tar".

The 0th and 1st (least significant) bits of the permissions field tell what kind of file it is.

Table: Input File Metadata Packet Permission Bits
Value	File Type	Additional data
0b00 	"normal" file	checksum of Input File Data Integrity packet
0b01	directory 	<none>
0b10	softlink	string containing name of file linked to 
0b11 	<not used>	<N/A>

A hardlink is encoded as two Input File Metadata Packets, each having file type "normal" and using the same checksum from Input File Data Integrity packet.  

The 3rd (least significant) bit of the permissions field is read permissions.  It is 1 if the user can read the file, 0 if not.  (It seems strange that a user could own a file but not read it, but this bit is included anyway.)

The 4th bit of the permissions field is write permissions.  It is 1 if the user can write the file, 0 if not.

The 5th bit of the permissions field is execute permissions.  It is 1 if the user can run the file, 0 if not.

If the encoding client's filesystem does not support the metadata, the read, write and execute bits should be set to 1.  The create and last modification times should be set to the current time.  

If the decoding client's filesystem does not support the metadata used inside the PAR 3.0 file, the client should do their best to approximate them and inform the user.  For example, the FAT filesystem has shorter filenames, no "execute" bit and low-resolution timestamps.  In this case, a client might force a shorter filename, ignore the execute bit, set the timestamps to the closest value, and inform the user that those changes were made.  


Input Subset Metadata Integrity Packet

This packet type is used to group all input files and their metadata.  It works similarly to the Input Subset Data Integrity packet.  

These packets can form a tree.  The checksum of the root packet is placed in the Root Packet.

The input subset data integrity packet has a type value of "PAR 3.0\0ISMI\0\0\0\0" (ASCII). The packet's body contains the following:

Table: Input Subset Metadata Integrity Packet Body Contents
Length (bytes)	Type	Description
?*16   K12 hash		The checksums of Input File Metadata Integrity packet OR Input Subset Metadata Integrity packets.

If there is no parent stream, it is recommended that the files appear in file name order.  It is also recommended that the tree of these packets be "broad" and not "deep".  


Root Packet

This packet contains the checksum at the root of the hierarchy of Input Subset Data Integrity or Input Subset Metadata Integrity packets.  It represents the group of files in the stream.

The root packet has a type value of "PAR 3.0\0Root\0\0\0\0" (ASCII). The packet's body contains the following:

Table: Root Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum of a Input Subset Data Integrity or Input Subset Metadata Integrity packets.

A stream can only contain one Root packet, although it may contain multiple copies of the same packet.  



Recovery Data Packet

This packet contains a recovery block.

The recovery data packet has a type value of "PAR 3.0\0Recovery" (ASCII). The packet's body contains the following:

Table: Recovery Data Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the Matrix packet
16	K12 hash 	The checksum from the Root packet
8	unsigned int	The index of the row
?*8	data	 	The recovery block data

The recovery block data is calculated using the code matrix as determined by the Matrix packet checksum.  Which row of the matrix is used is determined by the row index.  (Remember, the row indices start at zero.)  The data in the single virtual file is determined by the Root packet.  

Clients that calculate the recovery data will want to traverse the list of the input data files to find the last byte in the single virtual file that is mapped to a data file.  This can be used to figure out how many columns of the code matrix are used and need to be calculated.  (Mathematically, it works out that if the code matrix infinite, because any unspecified parts of the single virtual file are specified to be zero bytes and won't change the recovery data.)  


Input File Hint Packet

This packet is used to support single-pass recovery.  It contains data like the filename and mapping of data to the single virtual file.  It data is not protected by the integrity packets and is only a hint to decoding clients.

The input file hint packet has a type value of "PAR 3.0\0IFHint\0\0" (ASCII). The packet's body contains the following:

Table: Input File Hint Body Contents
Length (bytes)	Type	Description
8      unsigned int 	File index
8      unsigned int 	Filename length
?*8    string		Filename, plus 0 to 7 zero bytes of padding
16	unsigned int	File size
?*48	{index in input file, index in single virtual file, length} array	mapping of input file to single virtual file.  (See below.)
32	K12 hash	zero bytes OR K12 hash of the first 16kibibytes of protected section of the file 

The filename length and filename should be the same as in the Input File Metadata packet.  If a file has multiple "hard links" and, as a result, multiple Input File Metadata packets, there should be only one Input File Hint Packet.  The encoding client should pick just one filename.  The decoding client can create the hard links when it reads the multiple Input File Metadata packets.

The file size and mapping data should be the same as in the Input File Data Integrity Packet.

The final hash is only on the first 16kibibytes of the protected data of the file.  The hash is used to find files that have been renamed.  The hash is only computed on the first 16kibibytes that is mapped to the single virtual file.  If less than 16kibibytes is mapped, then all of the data is hashes.  Like the hash in the Input File Data Integrity packet, any unmapped data is skipped.  If the encoding client does not want to use this feature, the hash is set to all zeroes.  


Use as a Streaming Protocol:

The PAR 3.0 packet format can be used as a streaming protocol to deliver data objects with forward error correction.  In this case, metadata packets are never used.  A completed object is indicated by a Input File Data Integrity packet.

If a group of objects must be sent as a unit, the Root packet indicates the end of the group.  If any more data must be sent after the group, the sender creates a new Stream ID and, possibly, set the parent fields of Single Virtual File packet to the previous Stream ID.  


Conventions:

To make sure clients work similarly, the following client conventions should be followed.

When supporting single-pass recovery, the order of packets should be:
  Creator packet
  Input File Hint packets (one for each input file with protected data)
  Single Virtual File packet
  Data packets or Data Checksum packets
  Input File Integrity packets (one for each input file with protected data)
  Input Set Integrity packet
  Input File Metadata packets (one for each input file)
  Input Set Metadata Integrity packet
  Recovery Data packets

The decoding PAR client can recovery the information in Data packets and Recovery Data packets, but not in any of the other packets.  It is necessary to repeat those packets multiple time, possibly copied in multiple files or multiple times in the same file.  

PAR 3.0 files should always end in ".par3". For example, "file.par3". If a file contains recovery data packets, the ".par3" should be preceded by ".volXX+YY" where XX is the matrix row of the first recovery block contained in the file and YY is the number of recovery blocks in the file. For example, "file.vol20+10.par2". More than 2 digits should be used if necessary. Any row numbers that contain fewer digits than the largest row number should be preceded by zeros so that all filenames have the same length. For example, "file.vol075+50.par2". Row numbers should start at 0 and go upwards.

If multiple PAR files are generated, they may either have a constant number of slices per file (e.g. 20, 20, 20, ...) or exponentially increasing number of slices (e.g., 1, 2, 4, 8, ...). Note that to store 1023 slices takes 52 files if each has 20 slices, but takes only 10 files with the exponential pattern.

When generating multiple PAR files, it is expected that one file be generated without any Input File Hint, Data, or Recovery Data packets and containing all the other packets. The other files can duplicate this information, as a way to repeat data that cannot be recovered.

Recall that all files must contain a creator packet.

It is recommended that users are warned when they create PAR files with names that are incompatible with Windows, Mac, or Linux systems. That is, file or directory names that are more than 255 characters long, start with a period (.) or a dash (-), or contain one of these characters: < > : " ' ` ? * & | [ ] \ ; or newline (\n).

It is strongly recommended that clients query a user before writing to a file whose File Description packet contains an absolute pathname. For Windows, that means one starting with "C:\" or "//" for example. For UNIX, that means one starting with "/" or "//". For Mac, that means one starting with ":". This is to prevent PAR files of unknown origin from cracking a system by overwriting system files. 







Questions:

Is 64-bit file size enough?  Wikipedia shows hard drive size increasing by a factor of 1000 every 15 years or so.  Another way to view it is doubling every 1.5 years.  If 32-bits was exceeded in 1990, then 64-bit will be exceeded around 1.5*32 years later, or 2038.  Given it's been 20+ years between Par2 and Par3, I don't think 64bits is enough.  Certainly not for the single virtual file.



File hash  (license, code, projects using, speed, ...)
  Blake3 :
    8 times faster than MD5, using single thread SSE, 16kB input
       ---> Blake3 paper says it is roughly the same speed, maybe a touch faster, than KangarooTwelve
       ---> same paper says it is much faster on an ARM (Raspberry Pi)
       ---> VERY multi-threadable
    Public Domain CC0 1.0
    Rust is default implementation; C doesn't use threads.
    GCC or MSVC
    256-bit output
  KangarooTwelve:
    Mostly Public Domain CC0
    Python or Rust or C
    GCC  (MSVC support is experimental)
    Variable sized output, suggested 128-bit


K12
  -- require xsltproc




NOTE: non-systematic linear codes

WARNING: Unicode filenames sometimes use 1 or 2 characters for umlaut, circomflex, ...  "diaeresis"





Use cases:

File distribution, with separate Par3 file:

User wants to send a set of files on Usenet.  They use a Par3 client to generate redundant data in a separate file.  They send the input files and redundant file over Usenet.

The receiver downloads files from Usenet.  Uses Par3 client to verify files and, if any are damaged, recover the damaged files.  


File distribution, with data inside Par3 file:

User wants to distribute a set of files together and the package might go out over various transport protocols (website, usenet, etc.).  They use a program like "tar" or "zip" to group the files.  They then use a Par3 client to add redundancy to the single file.

The receiver downloads the file.  They try the archive program to unpack the files.  If the user detects a problem, they can use a Par3 client to repair the file before retrying the archive program.


Backup, with separate Par3 file:

User wants to backup files.  They use an archiver, like "tar" to create one or more archive files.  They run a Par3 client to create redundant data in a separate file(s).  The archive file(s) and Par3 file(s) are stored.

If the user sees a problem, they can restore from the archive file(s).  If the archive file(s) are damaged, they use a Par3 client to repair the original archive file(s).  They then use the archiver program to restore the original files.


Backup, with data inside Par3 file:

The user runs an archiver to generate archive file(s).  The Par3 client is used to group the archiver output, calculate redundant data, and split the data into multiple output files.  Those output files are then stored.

If the user sees a problem, they run the Par3 client, which outputs the original archive files.  They then run the archiver to restore the backed up files.


Incremental backup:

After having done a full backup, with Par3 data in a separate file, the user desires to do an incremental backup.  They use the archive program to generate an archive file containing the incremental changes.  The user then runs a Par3 client on the archive file(s) of the full backup and the archive file(s) of the incremental backup.  The redundant data is calculated and written to a separate file.

If the user sees a problem, they use a Par3 client to verify and, optionally, repair the full archive file and the incremental archive file.  They then run the archiver to retore the original files.


No-backup redundancy:

User wants to protect important file(s) from damage/accidental deletion.  They use a Par3 client to generate redundant data in a separate file.

If an important file is lost/damaged, the Par3 client reads the redundant data file and any existing original files and attempts to recover the important file.  If many other files have changed, recovery may not be possible.


File streaming:

User wants to transmit file(s) over a one-way connection, with forward error correction.  For example, over UDP or using multicast UDP.  The user include a Par3 library in their program.  The file is packaged into Par3 packets, which are sent to the receiver.

The receiving program also includes a Par3 library.  After receiving UDP packets, they are passed to the library, which writes the files.  When a complete file is received, the receiving program is notified and passed the data.  

NOTE: This use case is file-based.  I don't think we can support stream-based operation, because Par3 clients do recovery on fixed block size.  A user could write the program so that, instead of flushing the stream, it closes the current file and sends it to the client and then starts a new file.  (I actually think that's a better semantic than streaming.)





