Parity Volume Set Specificiation 3.0

Michael Nahas

Started January 16th, 2020


Based on Parity Volume Set Specification 2.0 [2003-05-11] by Michael Nahas with ideas from Peter Clements, Paul Nettle, and Ryan Gallagher
Based on Parity Volume Set Specification 1.0 [2001-10-14] by Stefan Wehlus and others.


Introduction:

This document describes file formats for storing redundant data for a set of files.  If any of the original files is damaged in storage or transmission, the redundant data can be used to regenerate the original input.  Of course, not all damages can be repaired, but many can. 

This document describes two file formats, which work together.  These formats also work as network protocols, so instead of storing and retrieving files, the network protocol would send and receive data objects.  For simplicity, they are described as file formats.

The "PAR 3.0 Recovery" file format stores redundant data for a single file.  If the file is damaged in storage or transmission, the redundant data can be used to regenerate the original input.  This file format also allows a large file to be broken into pieces, which are stored on separate disks.  (This may be more useful for a network protocol, where data objects are broken into separate packets or streams.)

The "PAR 3.0 Archive" file format says how to group multiple files into a single file.  The single file stores not just file contents but also metadata, like the filenames and (some) permissions.  The format is designed to be resilient --- if the single file is damaged, most of the files/data can still be retrieved.

Used together, these file formats allow a set of files to be archived securely.  Most forms of damage can be repaired.  Damage that is not repairable, is at least partially recoverable.


Terms:

In operation, a user will select a set of files from which the redundant data is to be made.  These are known as "input files" and together they are known as the "input set". The user will provide these to a program which generates files that match the specification in this document.  The program is known as a "PAR 3.0 client" or "client" for short, and the generated files are known as "PAR 3.0 files" or "PAR files". If the files in the input set ever get damaged (e.g. when they are transmitted unreliably or stored on a faulty disk) the client can read the damaged input files, read the (possibly damaged) PAR files, and regenerate the original input files.  Again, not all damages can be repaired, but many can. 

If any aspect of the format is specific to network protocols, the language will switch from "files" to "data objects".  The only difference is that "data objects" do not have metadata, such as filenames and permissions.  (As you will see, each file's metadata is just treated like another "data object" to simplify the design.)



Design Goals:

PAR 3.0's goal is to provide a complete solution for the bottom two layers of archiving: redundant data and splitting.  Other layers --- like compression, encryption, and storing metadata --- may be better served by other programs (zip/gzip/7zip, pgp/gpg, tar, etc.)  PAR 3.0 does provide minimal support for most other layers, for ease and integration.

PAR 3.0 does not provide support for encryption, because it is tricky to do well.  PAR 3.0 does not support ownership of files, because of concerns about security.  

Major differences from PAR 2.0 are:
* support any systematic linear code (Reed-Solomon with Vandermonde matrix or Cauchy, LDPC, random sparse martix).
* support single-pass recovery when no errors are encountered.
* support files that work both as a PAR 3.0 file and another type.  For example, putting recovery data inside a ZIP, ISO 9600, or other file.

Part of "support any systematic linear code" is to fix the major bug in PAR 2.0: it did not do Reed-Solomon encoding as it promised.  There was a major mistake in the paper that PAR 2.0 relied on.  The problem manifested as a bug in PAR 1.0 and, while PAR 2.0 reduced its occurance, it did not fix the problem.  PAR 2.0 did not use an always invertible matrix; it essentially used a random matrix, which (luckily) is invertible with high probability.  PAR 3.0 fixes that bug.

The other part of "support any systematic linear code" is that non-Reed-Solomon codes can be much faster.  LDPC and sparse random matrices will speed things up dramatically, with a slight increase in errors that cannot be recovered from.

Some minor differences from PAR 2.0 are:
* UTF-8 filenames (which were supported as a never-published PAR 2.1 standard)
* support for recovering multiple PAR files with overlapping input file sets
* the ability to change file names without regenerating every packet
* support for empty directories
* support for more than 2^16 files

(NOTE: A major design decision was to focus on file format that could also be used as a network protocol.  The alternative was to focus on a file format that could also be used as a filesystem.  E.g., a derivative of BTRFS with an additional tree of redundancy blocks.  That approach is an interesting direction, for any open source designers out there.)


The Math of Redundancy

The major feature of PAR is to support redundant data.  PAR uses Linear Algebra to generate the redundant data and, after damage, to recover the original input data.  To understand how it works, you need to be familiar with vectors, matrices, etc.

The calculation of redundant data starts with the input data being packaged into a set of vectors.  Those vectors are multiplied by the "code matrix" to generate vectors of redundant data.  Thus, an input vector "i" times the code matrix "C" generates a redundant vector "r".

r = iC

Recovery is accomplished by first identifying the good input data and good redundant data.  Good data is data that arrived intact; bad data was lost or damaged.  We can then permute and partition of the elements of each vector into "good" ones and "bad" ones.  We can do the same for the elements of the code matrix.

r = | r_good |   i = | i_good |    C = | C_good,good C_good,bad | 
    | r_bad  |       | i_bad  |        | C_bad,good  C_bad,bad  |

| r_good | = r = iC = | i_good || C_good,good C_good,bad |
| r_bad  |            | i_bad  || C_bad,good  C_bad,bad  |            


Our goal, of course, is to recover the bad input data.  To do that, we pull out the equation for the good redundant data...

r_good = i_good*C_good,good + i_bad*C_good,bad

... and solve for the bad input data.

i_bad = (r_good - i_good*C_good,good) C_good,bad^-1

Here, "^-1" indicates the right inverse of the matrix.  Since the redundant data is made by multiplying the input data by the code matrix, the inverse of the code matrix allows us to recreate the missing input data from the redundant data.  Not every matrix has a right inverse.  When the right inverse does not exist, we cannot recover the input data.  A right inverse never exists when the matrix has more columns than rows, which means we cannot recover if there are more bad input blocks than good redundant blocks.  

Unlike your linear algebra class, the elements of the vectors and matrices are not real numbers or complex numbers, but elements of a "Galois Field".  Like the computer's integers (a.k.a., the integers modulo 2^N), they come in various sizes like 8-bits, 16-bits, etc. and support operations called addition, subtraction, multiplication, and division.  Unlike the computer's integers, "division" exactly inverts "multiplication" for every value.  (Computer integers can overflow during multiplication, preventing division from inverting the multiplication.)  That perfect inversion allows the linear algebra to work.

PAR 3.0 improves on PAR 2.0 by supporting multiple Galois Fields and by supporting any code matrix.  This means PAR 3.0 supports a large set of error correcting codes known as "linear systematic codes".  These include Reed-Solomon and many Low Density Parity Check (LDPC) codes.  This flexibility allows PAR 3.0 clients to choose between speed and the number of errors that can be recovered.  It also allows PAR 3.0 to support any codes whose patents expire or new codes that are developed.



Specifics of Computation 

This section goes into the details of how redundant data for a single file is computed and how recovery proceeds.  That is, how the mathematical vectors used in the previous section related to actual bytes in a file.

Generating the recovery data starts with choosing a Galois Field and block size.  The Galois Field is usually chosen based on whatever is fastest for the computer's hardware.  For this example, I'll assume it fits in 2-bytes (16-bits).  The block size is the smallest unit for recovering data.  It is usually chosen to match the transmission/storage technology or to limit overhead.  The block size must be a multiple of the size of the Galois Field.  For this example, I'll assume it is 2048 bytes, but in practice it can be much larger, often megabytes.

First, the single input file is broken into equal-sized blocks.  If a block isn't completely filled with data from the file, the rest of the block is assumed to be filled with zero bytes.  For this example, the blocks are each 2048 bytes long.  If the single file (with zero byte padding) is N blocks long, each of those N blocks can be seen as holding 1024 Galois Field values (each 2-byte in size).  

The next step reorganizes the blocks into vectors.  The N blocks containing 1024 Galois Field values becomes 1024 vectors containing N Galois Field values.  This is done the obvious way: swapping rows for columns and columns for rows.  The values in the i-th block become the i-th element of each vector; the j-th values in each block are used to make the j-th vector.   

Next, the user chooses the numbers of recovery blocks that they want. The number of recovery blocks determines the maximum number of damaged/missing input blocks that we can recover.  Often the number of recovery blocks is 5% or 10% of the number of input blocks. 

Next, is the choice of the code matrix.  The code matrix has a column for each input block and a row for each recovery block.  The elements of the matrix can be anything --- PAR 3.0 supports any systematic linear code.  Codes vary in speed and the probability of recoverying from an error.

Next, for each input vector, we make a recovery vector by multiplying the input vector with the code matrix.  There is only one code matrix; the code matrix is the same for every pair of vectors.  Since there is one recovery vector for each input vector, there are 1024 recovery vectors.  The length of each recovery vector is equal to how many recovery blocks we want.  

The next step reorganizes the recovery vectors into recovery blocks.  It is basically the inverse of the step that reorganized the input blocks into input vectors.  If we want R recovery blocks, the 1024 recovery vectors of length R become R recovery blocks with 1024 Galois Field values.  The i-th element in each vector goes into the i-th recovery block; the j-th vector is used to make the j-th value in each recovery block.

The recovery blocks are stored in a PAR 3.0 file or multiple PAR 3.0 files.  The input blocks can be stored in their original files or put in PAR 3.0 file(s).  The PAR 3.0 files contain a checksum for every input and recovery block.  After the files are stored or transmitted, the checksums are used to determine which blocks are damaged.

After storage or transmission, if there are any input blocks missing or damaged, we need to do recovery.

The first step of recovery is identify elements of the input vectors associated with the missing/damaged input blocks.

Next, we identify elements of the recovery vectors associated with the missing/damaged recovery block.

We then perform the math (described above) which uses the good elements of each recovery vector to recover the bad elements of its associated input vector.  The math requires inverting a submatrix of the code matrix and, if the right inverse does not exist, we fail.

After recovering the missing elements of each input vector, the data is reorganized to regenerate the missing input blocks.  Those blocks are written into the single file, which should complete the repair.  The PAR 3.0 file can contain a checksum for the file to make sure that the repair process worked correctly.



Conventions:

There are a number of conventions used in this specification.

The data is 8-byte aligned. That is, every field starts on an index in the file which is congruent to zero, modulus 8.  (That is, address % 8 == 0)  This is because some memory systems function faster if 64-bit quantities are 8-byte aligned.  Note that a file could be corrupted (bytes inserted or deleted) and thrown off the alignment. 

All integers in this version of the spec are integers of 4, 8, or 16 bytes in length.

All integers are little endian. (This is the default on x86 and x86-64 CPUs.)  Signed integers are in 2's complement notation.  (This is the default on every major architecture.)  

Strings are not null-terminated.  This is to prevent hackers from using stack-overflow attacks.  In order to make a string 8-byte aligned, 1 to 7 zero bytes may be appended.  If an N-byte field contains an array, a null-terminated string can be created by copying the N-byte field into a character array of length N+1 and then the setting the N+1 character to '\0'.

The lengths of arrays and strings are often implicit.  For example, if a region is known to be 32 bytes and that region contains an 8-byte integer and a string, then the string is known to take up 24 bytes.  The string is then at least 17 bytes in length, since the 24 bytes contains 0 to 7 bytes of '\0' padding at the end.

All strings are UTF-8.  WARNING: Writers of OSX/MacOS clients must take special care with UTF-8 filenames!  Unicode has multiple ways to encode the same string.  An e with an accent mark can be encoded as a single character (U+00e9) or two characters, one for the e (U+0065) and one for the accent mark (U+0301).  PAR 3.0 does not require a particular encoding.  Forcing a particular encoding is called "normalization" in the Unicode vocabulary.  Most file systems do not normalize filenames and just treat the UTF-8 as a sequence of bytes.  PAR 3.0 follows their practice.  However, HFS+ was Apple's default filesystem from 1998 to 2017 and it normalizes every filename.  Thus, if a PAR client writes a file with a UTF-8 filename, the HFS+ file system may change the filename.  Clients for OSX/MacOS should be aware of this possibility.  Apple's current default filesystem, APFS, does not do normalization.  

The lengths of files and locations in files are determined by 16-byte integers. 
(Hard drive size doubles every 1.5 years and is expected to exceed 8-byte integers before 2040.)  It is acceptable for clients to not support files that are larger than 2^64.  Clients that do not support large files must inform the user if they encounter a PAR file containing too large a file.

The block size and matrix indices are 8-bytes integers.  In order to protect files with more than 2^64 bytes, users must choose larger block sizes.  

PAR 3.0 uses multiple hash functions.  The "rolling hash" is CRC32C.  It is used to identify input blocks that are not in their expected location.  This hash is only 32 bits long and may not uniquely identify a block.  

For uniqueness, PAR 3.0 uses the KangarooTwelve hash, a.k.a. K12 hash.  This is a longer hash with cryptographic properties.  The K12 hash of a block or file is assumed to be unique.

Every byte of a PAR file is specified. There are no places to throw junk bytes that can be any value.  Padding, where needed, is specified to be zero bytes.  The order of items in all arrays is specified.

When discussing vectors and matrices, this document uses zero-indexing.  That is, the elements in a vector are at locations 0 through N-1.  (One-indexing, the usual convention, has them at locations 1 through N.)



PAR 3.0 Recovery File Format

The PAR 3.0 Recovery file format supports redundant data for a single file and splitting the output into multiple small files.  It only operates on the contents of the single file --- it does not store any metadata.


Packets

A PAR 3.0 Recovery file itself is made of packets - self-contained parts with their own checksum. This design prevents damage to one part of the file from making the whole file unusable. 

Packets have a type and each type of packet serves a different purpose.  One type describes the code matrix.  Another contains input blocks.  Yet another contains a recovery block.  There are many other types.

A PAR 3.0 Recovery file is only required to contain 1 specific packet - the packet that identifies the client that created the file. This way, if clients are creating files that don't match the specification in some way, they can be tracked down.

Packets can be packaged into multiple files.  Each packet contains a Stream ID, which is used to identify different files that contain data from the same original file.  

Packets can be duplicated, in different files or even in the same file.  In fact, duplicating packets is recommended for vital packets, such as the ones the code matrix.

Packets can appear in any order in a file, but there is a recommended order that allows some clients to recover the file(s) in a single pass.  



Packet Header

A PAR 3.0 Recovery file consists of a sequence of "packets". A packet has a fixed-sized header and a variable length body. The packet header contains a checksum for the packet - if the packet is damaged, the packet is ignored. The packet header also contains a packet-type. If the client does not understand the packet type, the packet is ignored. To be compliant with this specification, a client must understand the "core" set of packets. Client may process the optional packets or create their own application-specific packets.


Table: Packet Header
Length (bytes)	Type	Description
8	byte[8]	Magic sequence. Used to quickly identify location of packets. Value = {'P', 'A', 'R', '3', 'R', 'E', 'C', '\0'} (ASCII)
8	8-byte uint	Length of the entire packet. Must be multiple of 8. (NB: Includes length of header.)
16	K12 hash	K12 Hash of packet. Used as a checksum for the packet. Calculation starts at first byte of Stream ID and ends at last byte of body. Does not include the magic sequence, length field or this field.  
16	K12 hash	StreamID.  All packets that belong together have the same Stream ID. (See below for how it is calculated.)
16	byte[16]	Type. Can be anything. All beginning "PAR " (ASCII) are reserved for specification-defined packets. Application-specific packets are recommended to begin with the ASCII name of the client.
?*8	?	Body of Packet. Must be a multiple of 8 bytes.

The StreamID is used to identify packets that should be processed together, even if those packets were written to separate PAR 3.0 Recovery files or arrived via different transmission methods.  A StreamID is associated with a unique single file together with the block size and Galois Field.  If the contents of the single file are known, it could be a K12 hash of the single file along with the block size and Galois Field parameters of the Basics packet.  This is impossible in streaming or single-pass situations.  In those cases, it should be a globally unique random number.  One way to generate the random number is the K12 hash of the triple consisting of: a machine identifier, a process identifier, and a high-resolution timestamp.

There are various types of packets. The "core" set of packets - the set of packets that all clients must recognize and process - are listed next. For each, the value for the "type" field will be listed along with the contents of the body of the packet. 


Creator packet

This packet is used to identify the client that created the file.  It is required to be in every PAR 3.0 Recovery file.  If the decoding client is unable to recover the input set due to a badly created file, the contents of the creator packet must be shown to the user.  The goal of this is that any client incompatibilities can be found and resolved quickly.

The creator packet has a type value of "PAR 3.0\0Creator\0" (ASCII). The packet's body contains the following:

Table: Creator Packet Body Contents
Length (bytes)	Type	Description
?*8	UTF-8 char array	UTF-8 text identifying the client.  This should also include a way to contact the client's creator - either through a URL or an email address.  Reminder: This is not a null terminated string.

It is recommended that the text in the creator packet include any parameters used to generate the file.  For example, the command line arguments.  This will aid in debugging problems.


Basics packet

This packet sets the block size and Galois Field.  It also identifies the parent stream, if there is one.

The Basics packet has a type value of "PAR 3.0\0Basics\0\0" (ASCII). The packet's body contains the following:

Table: Basics Packet Body Contents
Length (bytes)	Type	Description
8	8-byte uint	The size of the Galois field in bytes.
?*8	?-byte GF	The generator of the Galois field without its leading 1.
8	8-byte uint	block size in bytes
16      K12 hash        Stream ID of parent stream, or all zeros.

The second entry in the packet holds an element of the Galois field.  It is written in little-endian format and padded with 0 to 7 zero bytes afterwards.  Thus, if the Galois field had a size of 2-bytes and a generator of 0x1100B, the entry's first two bytes would hold the value 0x100B in little-endian format and the next 6 bytes would be zeroes.  Notice that the entry does not encoded the leading 1 that is present in the mathematical notation of the generator.  

Clients must support every possible Galois field. 

Clients are expected to optimize performance for specific Galois fields.  Some likely targets for optimization are:

Table: Common Galois Fields
Size (bits)    Generator (hexidecimal)
8              0x11B 
16             0x1100B
128            (1 << 128) + 0x43 

Note: The 8-bit Galois field is supported by the x86 instruction GF2P8MULB.

Note: The 16-bit Galois field is the same as in Par 2.0.

Note: 64-bit Galois fields are supported by the x86 instruction CLMUL.

Note: The 128-bit Galois field is implemented by Intel in this white paper:
https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/carry-less-multiplication-instruction-in-gcm-mode-paper.pdf

Note: The ARM processor has an instruction extension called "NEON" with a VMULL instruction.  VMULL.P8 will do eight 8-bit Galois field multiplications at once.

The block size must be a multiple of 8.  It must also be a multiple of the Galois Field size. 

The "parent stream" concept will mostly be used for incremental backups.  In this use case, there is an original backup of all files and then a incremental backup containing changes since the original.  The incremental backup is a different stream, but might want to reuse data from the original backup.  This works by appending the new data to the end of single file in the parent stream.  Its blocksize and Galois field must be the same as the parent's.  The parent stream must contain a Checksum packet, which holds the length of the parent's input file.

For any Stream ID, there should be only 1 Basics packet.  (The same packet may be repeated multiple times.)


Matrix Packets

There are currently 3 different types of matrix packets.  More than one packet can be used at the same time.  For example, a sparse matrix packet could be used to generate most recovery data and a Cauchy matrix packet to generate a few pieces of recovery data.  (This dual approach balances speed and recovery of errors.)

Cauchy Matrix Packet

This packet describes a Cauchy matrix.  It is used for all or part of the code matrix.

The Cauchy matrix creates the best possible recovery data.  With the Cauchy matrix, any submatrix that can have a right inverse does have a right inverse.  (That is, any submatrix which doesn't have more columns than rows, has a right inverse.)  Using just one Cauchy matrix creates a Reed-Solomon code.  

The Cauchy matrix packet has a type value of "PAR 3.0\0Cauchy\0\0" (ASCII). The packet's body contains the following:

Table: Cauchy Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum of the Basics packet
8	unsigned int	Number of zero columns
8	unsigned int	hint for number of rows 

The hint to the number of rows is used in single-pass situations to allocate buffers.  If the number of rows is unknown, the hint is set to zero.

To generate the element at zero-indexed row R and zero-indexed column C, the first check is if C is less than the number of zero columns.  If it is, then the element is zero.

Otherwise, the element at zero-indexed row R and zero-indexed column C depends on R and C.  Specifically, it is the multiplicative inverse of x_R-y_C, where x_R is the Galois field element with the same bit pattern as binary integer R+1 and y_C is the Galois field element associated with the binary integer MAX-C, where MAX is the maximum binary integer value with the same size as the Galois field.  (NOTE: In binary, MAX contains all ones.)  To be clear, the multiplicative inverse and subtraction x_R-y_C are done using Galois field arithmetic.  The R+1 addition and MAX-C subtraction is done using native integer arithmetic.


Sparse Random Matrix Packet

This packet describes a sparse random matrix.  It is faster to calculate recovery blocks with a sparse matrix, if the user is willing to accept a slightly higher chance of not recoverying from extreme cases.

To be specific, consider a sparse random matrix with N recovery blocks such that the matrix has at least N*ln(N) non-zero elements.  This matrix is able to recover from almost all cases where N - C input blocks are lost.  The probability of failing for a given C is propotional to 1/(g^C) where g is the number of unique the Galois field values.  Thus, for a 1-byte Galois field with 256 values, the probably that recovery fails with N-3 lost input blocks, is 1/(256^3) or less than one-in-a-million.  See "The Rank of Sparse Random Matrices over Finite Fields" by Blomer, Karp, and Welzl.

Recall that for any matrix, N+1 lost input blocks can never be recovered.  Using a sparse matrix, rather than a Cauchy, can be upto N/ln(N) times faster.  Users that can store/send 6 additional recovery blocks can get much faster performance with less than a 1-in-a-trillion increase in failure to recover.

The speed and probability of recovery are better for large numbers of recovery blocks.  When there are few recovery blocks, the Cauchy matrix is strongly recommended.

The matrix rows packet has a type value of "PAR 3.0\0Sparse\0\0" (ASCII). The packet's body contains the following:

Table: Sparse Random Matrix Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the Basics packet
8	unsigned int	number of zero columns
8	unsigned int	number of rows 
8	unsigned int    lower bits of random number generator seed
8       unsigned int	number of non-zero elements per column

If the zero-indexed column number is less than the number of zero columns, then the column only contains zero Galois field values.

Otherwise, each column's non-zero elements are generated and then shuffled into position.  The following paragraphs describe the details.

First, the start of the column is filled with zeroes.  If the column has N elements (that is, the number of rows is N) and will have K non-zero elements, then the first N-K elements are set to zero.

Second, the pseudorandom number generator (PNRG) is initialized.  The generator is the PCG-XSL-RR, which has 128-bits of state and generates 64-bit pseudorandom numbers.  For each column of the matrix, the 128-bit seed is initialized with the higher 64-bits equal to the zero-indexed column of the matrix and the lower 64-bits equal to the seed value from the packet.

Third, non-zero elements are placed at the end of the column.  If there are K non-zero elements in a column, random non-zero Galois field values are placed in zero-indexed locations N-K-1 through N-1 of the column.  Random Galois field values are generated by filling them from the least-significant bytes to most significant using 64-bit values generated from the PRNG.  If the random value does not use all the bits of a value from the PRNG, the unused bits are discarded.  Since we want non-zero random values, if a zero is randomly generated, it is immediately discarded and whole new random values are generated.  Only when a non-zero value is generated do we move to the next location in the column.  This continues until the final K locations in the column have non-zero values.  

The last step in generating a random column is shuffling the non-zero values into their final location.  This is the usual O(N) shuffling algorithm, which swaps values into random locations.  (Wikipedia calls it "Fisher-Yates shuffle", others "Knuth shuffle".)  The first random location is in the range 0 to N-K-1 and is generated by taking a 64-bit unsigned value from the PRNG and applying modulus N-K.  The first non-random value, which is at location N-K-1, is then swapped with the value in the first random location.  (If the random location is N-K-1, then it "swaps with itself" and the column is not changed.)   The second random location is in the range 0 to N-K.  The second random non-zero value, at location N-K, is then swapped.  The third random location is in the range 0 to N-K+1.  Etc.  The final random non-zero value will be swapped to any location in the column.

The final result is a column with K random non-zero values in random locations.  This will mean data from from each input block will be incorporated into K different recovery blocks.  


Explicit Matrix Packet

This packet describes any single-row matrix.  The values of the elements are explicitly contained in the packet, and not computed like the other matrix packets.  It is intended to be used for LDPC, such as Tornado Codes.

Notice that this packet is row-based, while the sparse random matrix is column based.  The algorithm to repair LDPC is done on rows, rather than columns.  

The matrix rows packet has a type value of "PAR 3.0\0Explicit" (ASCII). The packet's body contains the following:

Table: Explicit Matrix Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the Basics packet
?*?	{8-byte unsigned int, GF value, padding}	zero-indexed column location, Galois field value, 0 to 7 bytes of padding

The matrix contains just one row.  For each non-zero element of the row, there is a triple in the packet with its location, value, and 0 to 7 bytes of zero padding to make it 8-byte aligned.  The triples are in sorted order, with column locations increasing from lowest to highest.



Data Packet

This packet contains the data contents of the single input file. 

The data packet has a type value of "PAR 3.0\0Data\0\0\0\0" (ASCII). The packet's body contains the following:

Table: Data Packet Body Contents
Length (bytes)	Type	Description
16	unsigned int	The byte offset in the single file
4	unsigned int	The compression format. (See below.)
12	K12 hash 	The hash of uncompressed data
?*8	unsigned int	The data itself. 

If the compression format code is 0, the data is uncompressed.  

If the compression format code is 1, the data is compressed with ZLIB.  

The uncompressed data's size must be a multiple of the blocksize.



Out-of-Stream Data Packet

This packet contains checksums for blocks that arrive by means other than a PAR 3.0 Recovery file.  It is used when the input file is sent along side the PAR 3.0 Recovery files.

The data packet has a type value of "PAR 3.0\0BlkChkSm" (ASCII). The packet's body contains the following:

Table: Data Checksum Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the Basics packet
16	unsigned int	The location of the first byte of the first block in the single input file.
?*16	{CRC32C, K12 hash} array	CRC32C and 12-byte K12 hash for a sequence of blocks in the single input file.  The CRC32C comes before the 12-byte K12 hash.

The CRC32C checksum is a rolling checksum.  It can be used to find block-sized pieces that have moved.

Notice that the K12 hash is only 12-bytes long here.  That is to keep the 8-byte alignment of data.  The K12 hash is used to uniquely identify the block, since CRC32C might not be unique.


Checksum Packet

This packet contains a checksum for the data of the input file. 

Table: Data Checksum Packet Body Contents
Length (bytes)	Type	Description
16	unsigned int	length of the input file
32	K12 hash	The checksum

This packet makes sure that the single input file has arrived correctly.  It may not be necessary if the input file contains its own checksums, like with PAR 3.0 Archive format.


Recovery Data Packet

This packet contains a recovery block.

The recovery data packet has a type value of "PAR 3.0\0Recovery" (ASCII). The packet's body contains the following:

Table: Recovery Data Packet Body Contents
Length (bytes)	Type	Description
16	K12 hash	The checksum from the Matrix packet
16	K12 hash 	The checksum from the Basics packet
8	unsigned int	The index of the row
?*8	data	 	The recovery block data

The recovery block data is calculated using the code matrix as determined by the Matrix packet checksum.  Which row of the matrix is used is determined by the row index.  (Remember, the row indices start at zero.)  The data in the single file is determined by the Basics packet.  

Clients that calculate the recovery data will want to traverse the list of the input data files to find the last byte in the single file that is mapped to a data file.  This can be used to figure out how many columns of the code matrix are used and need to be calculated.  (Mathematically, it works out that if the code matrix infinite, because any unspecified parts of the single file are specified to be zero bytes and won't change the recovery data.)  


Conventions:

To make sure clients work similarly, the following client conventions should be followed.

When supporting single-pass recovery, the order of packets should be:
  Creator packet
  Basic packet
  Matrix packets
  Data packets or Out-of-Stream Data packets
  Checksum packets
  Recovery Data packets

The decoding PAR client can recovery the information in Data packets and Recovery Data packets, but not in any of the other packets.  It is necessary to repeat those packets multiple time, possibly copied in multiple files or multiple times in the same file.  

PAR Recovery 3.0 files should always end in ".par3". For example, "file.par3". If a file contains recovery data packets, the ".par3" should be preceded by ".volXX+YY" where XX is the matrix row of the first recovery block contained in the file and YY is the number of recovery blocks in the file. For example, "file.vol20+10.par2". More than 2 digits should be used if necessary. Any row numbers that contain fewer digits than the largest row number should be preceded by zeros so that all filenames have the same length. For example, "file.vol075+50.par2". Row numbers should start at 0 and go upwards.

If multiple PAR files are generated, they may either have a constant number of blocks per file (e.g. 20, 20, 20, ...) or exponentially increasing number of blocks (e.g., 1, 2, 4, 8, ...). Note that to store 1023 blocks takes 52 files if each has 20 slices, but takes only 10 files with the exponential pattern.

Recall that all files must contain a creator packet.


End of description of PAR 3.0 Recovery file format

The PAR 3.0 Recovery file format only does recovery on the contents of a single file.  It can be used with any other file format to deliver the contents over a potentially unreliable network or a potentially faulty storage system.



PAR 3.0 Archive file format

The PAR 3.0 Archive file format is a simple archive file format.  It groups multiple files together.  For each file, it stores the metadata, including: filename, read/write/execute bits, and some timestamps.  If the file is damaged, most of the contents can be recovered.

It supports an incremental backup by appending to the archive file.

It supports being packaged inside the file it contains.  This is how we package PAR recovery data inside another file.  When this approach is done, only some of the containing file is able to be protected.

Layers

The file format has 2 layers.  The lower layer is concerned with the transfer of a group of data objects.  The upper layer interprets data objects as either metadata for a file or the contents of the file.

The lower layer could be reused with the PAR 3.0 Recovery file format as a network protocol that delivers a set of data objects with high reliability. 

Packets

Like the PAR 3.0 Recovery file format, the PAR 3.0 Archive file format has packets.  Files are made of packets: self-contained parts with their own checksum. This design prevents damage to one part of the file from making the whole file unusable.

Packets have a type and each type of packet serves a different purpose.  One stores some of the data for a data object.  Another holds the checksum for a data object.  Another holds the checksum for the entire group of objects.  There are many other types.

A PAR 3.0 Archive file is only required to contain 1 specific packet - the packet that identifies the client that created the file. This way, if clients are creating files that don't match the specification in some way, they can be tracked down.

A PAR 3.0 Archive file is a single file.  The format does NOT support data being split into multiple files, like PAR 3.0 Recovery file format.  Packets do contain a Stream ID, but this is to detect if a PAR 3.0 Archive file contains another PAR 3.0 Archive file inside it.  Clients only want to process data from the outermost stream.

Packets can appear in any order in a file, but there is a recommended order that allows some clients to recreate the input files in a single pass.  


Packet Header

A PAR 3.0 Archive file consists of a sequence of "packets".  A packet has a fixed-sized header and a variable length body. 

The packet header contains a checksum for some of the packet.  Unlike PAR 3.0 Recovery file format, this is not for the entire packet.  If the checksum does math the entire packet is ignored.  By only including some of the packet in the checksum, if the packet contents contain damaged file data, the decoding client can still written the damaged data out to the correct location of the correct file.

The packet header also contains a packet-type.  If the client does not understand the packet type, the packet is ignored. To be compliant with this specification, a client must understand the "core" set of packets. Client may process the optional packets or create their own application-specific packets.


Table: Packet Header
Length (bytes)	Type	Description
8	byte[8]	Magic sequence. Used to quickly identify location of packets. Value = {'P', 'A', 'R', '3', 'A', 'R', 'C', '\0'} (ASCII)
8	8-byte uint	Length of packet protected by the checksum.  Must be a multiple of 8.  
16	K12 hash	K12 Hash of packet. Used as a checksum for the packet. Calculation starts at first byte of Stream ID and ends at last byte of body. Does not include the magic sequence, length-protected-by-checksum field or this field.  
16	K12 hash	StreamID.  All packets that belong together have the same Stream ID. (See below for how it is calculated.)
8	8-byte uint	Length of the entire packet. Must be multiple of 8.
16	byte[16]	Type. Can be anything. All beginning "PAR " (ASCII) are reserved for specification-defined packets. Application-specific packets are recommended to begin with the ASCII name of the client.
?*8	?	Body of Packet. Must be a multiple of 8 bytes.

The StreamID is used to identify packets that should be processed together.  It is used to separate the packets when a PAR 3.0 Recovery file is stored inside another PAR 3.0 Recovery file.  Obviously, the outermost one should be processed.  If the contents of all the files are known, it could be a K12 hash of the K12 hashses of all the files in a known order.  This is impossible in streaming or single-pass situations.  In those cases, it should be a globally unique random number.  One way to generate the random number is the K12 hash of the triple consisting of: a machine identifier, a process identifier, and a high-resolution timestamp. 

There are various types of packets. The "core" set of packets - the set of packets that all clients must recognize and process - are listed next. For each, the value for the "type" field will be listed along with the contents of the body of the packet. 










Input files have a "file index", which is an 8-byte unsigned integer.  For most uses, the file indices will be 0, 1, 2, ... N-1.  The file index is used to differentiate between two files that have the identical contents, but are different files.  Files that do not contain data (directories, hardlinks, softlinks) do not have a file index.





